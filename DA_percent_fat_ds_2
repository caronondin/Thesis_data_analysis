####### this is the data analysis for "percent fat" using dataset 2###########

library(tree)
library(randomForest)
library(ggplot2)
library(gbm)
library(BART)

#opening the datasets 
dataset_1 <- read.csv('/Users/cnondin/Documents/THESIS/dataset/datasets for analysis /dataset_1.csv')
dataset_2 <- read.csv('/Users/cnondin/Documents/THESIS/dataset/datasets for analysis /dataset_2.csv')
dataset_3 <- read.csv('/Users/cnondin/Documents/THESIS/dataset/datasets for analysis /dataset_3.csv')


#RESULTS (MSE): 

#simple tree: 53.12201
#pruned:28.85202
#bagged tree: 20.37050 (tree = 300)
#rf: 20.28067 (tree = 100)
#boosted tree: 17.99876 (tree = 100, int depth = 3, shrink = 0.100)
#BART: 8.363738

#removing the percent fat column 
dataset <- subset(dataset_2, select = -fat)

#1.a.making a regression tree
set.seed(1)
train <- sample(1:nrow(dataset), nrow(dataset) *0.8)
simple.tree <- tree(percent_fat ~ ., dataset, subset = train)
summary(simple.tree) #you'll find the important variables here 
plot(simple.tree)
text(simple.tree, pretty = 0)

#testing the tree 
yhat <- predict(simple.tree, newdata = dataset[-train, ])
test <- dataset[-train, "percent_fat"]
plot(yhat, test)
abline(0, 1)
mean((yhat - test)^2)
#MSE = NA?

cv <- cv.tree(simple.tree)
plot(cv$size, cv$dev, type = "b")

#it seems like anything under 6 layers to the tree has the lowest missclassification error 

#1.b.pruned tree 
#can't do predictions with a depth of 1

# Define the range of depth values to test
depth_values <- c(2, 3, 4, 5, 6)

# Create an empty list to store the pruned tree models
pruned_models <- list()
mse_results <- numeric(length = length(depth_values))

# Iterate over each depth value and train the pruned tree model
set.seed(1)
for (i in seq_along(depth_values)) {
  depth <- depth_values[i]
  
  # Train the pruned tree model
  prune.tree <- prune.tree(simple.tree, best = depth)
  pruned_models[[as.character(depth)]] <- pruned_models
  
  # Make predictions on the test set
  yhat <- predict(prune.tree, newdata = dataset[-train, ])
  test <- dataset[-train, "percent_fat"]
  
  # Calculate mean squared error
  mse <- mean((yhat - test)^2)
  mse_results[i] <- mse
}

# Create a data frame for the ntree values and mean squared error
results <- data.frame(depth = depth_values, mse = mse_results)

# Create a line plot of the mean squared error
ggplot(results, aes(x = depth, y = mse)) +
  geom_line() +
  geom_point() +
  xlab("Tree Depth") +
  ylab("Mean Squared Error")

# depth of 2 is the best. MSE = 28.85202

#this is so you can take a look at the final model 
#you can make the model with depth of 2 here 
#plot(prune.ds_1.v2)
#text(prune.ds_1.v2, pretty = 0)

#1.c. random forest 

#making a new dataset with imputed NAs:
#I'm using random values to replace the NAs 
imp_data <- dataset

#changing anysupp variable 
observed_values <- imp_data$anysupp[!is.na(imp_data$anysupp)]
imp_data$anysupp[is.na(imp_data$anysupp)] <- sample(observed_values, size = sum(is.na(imp_data$anysupp)), replace = TRUE)

#changing cignow variable 
observed_values <- imp_data$cignow[!is.na(imp_data$cignow)]
imp_data$cignow[is.na(imp_data$cignow)] <- sample(observed_values, size = sum(is.na(imp_data$cignow)), replace = TRUE)

#changing season variable 
observed_values <- imp_data$season[!is.na(imp_data$season)]
imp_data$season[is.na(imp_data$season)] <- sample(observed_values, size = sum(is.na(imp_data$season)), replace = TRUE)

#changing education variable 
observed_values <- imp_data$education[!is.na(imp_data$education)]
imp_data$education[is.na(imp_data$education)] <- sample(observed_values, size = sum(is.na(imp_data$education)), replace = TRUE)

#changing ethnicity variable 
observed_values <- imp_data$ethnicity[!is.na(imp_data$ethnicity)]
imp_data$ethnicity[is.na(imp_data$ethnicity)] <- sample(observed_values, size = sum(is.na(imp_data$ethnicity)), replace = TRUE)

#creating a new training set 
set.seed(1)
imp_train <- sample(1:nrow(imp_data), nrow(imp_data) *0.8)
imp_test <- dataset[-imp_train, "percent_fat"]

#bagged tree 
set.seed(1)
bag.tree <- randomForest(percent_fat ~., data = imp_data, subset = imp_train, mtry = 2207,
                         importance = TRUE) #mtry = p for bagged tree 
yhat.bag <- predict(bag.tree, newdata = imp_data[-imp_train, ])

plot(yhat.bag, imp_test)
abline(0, 1)
mean((yhat.bag - imp_test)^2) #MSE = 21.01218

#trying bagged tree with different tree depth 
# Define the range of ntree values to test
ntree_values <- c(100, 200, 300, 500, 1000, 2000, 3000, 4000, 5000)

# Create an empty list to store the random forest models
rf_models <- list()
mse_results <- numeric(length = length(ntree_values))
# Iterate over each ntree value and train the random forest model
set.seed(1)
for (i in seq_along(ntree_values)) {
  ntree <- ntree_values[i]
  
  # Train the random forest model
  rf_model <- randomForest(percent_fat ~ ., data = imp_data, subset = imp_train, mtry = 2207, ntree = ntree)
  rf_models[[as.character(ntree)]] <- rf_model
  
  # Make predictions on the test set
  yhat.rf <- predict(rf_model, newdata = imp_data[-imp_train, ])
  
  # Calculate mean squared error
  mse <- mean((yhat.rf - imp_test)^2)
  mse_results[i] <- mse
}

# Create a data frame for the ntree values and mean squared error
results <- data.frame(ntree = ntree_values, mse = mse_results)

# Create a line plot of the mean squared error
ggplot(results, aes(x = ntree, y = mse)) +
  geom_line() +
  geom_point() +
  xlab("Number of Trees") +
  ylab("Mean Squared Error")

#the best one was tree = 300, MSE = 20.37050

#random forest 
set.seed(1)
rf <- randomForest(percent_fat ~ ., data = imp_data,
                   subset = imp_train, mtry = 736, importance = TRUE) #mtry = p/3 in rf, default ntree = 500
yhat.rf <- predict(rf, newdata = imp_data[-imp_train, ])
mean((yhat.rf - test)^2) #MSE = 22.29078

# Define the range of ntree values to test
ntree_values <- c(100, 200, 300, 500, 1000, 2000, 3000, 4000, 5000)

# Create an empty list to store the random forest models
rf_models <- list()
mse_results <- numeric(length = length(ntree_values))

# Iterate over each ntree value and train the random forest model
set.seed(1)
for (i in seq_along(ntree_values)) {
  ntree <- ntree_values[i]
  
  # Train the random forest model
  rf_model <- randomForest(percent_fat ~ ., data = imp_data, subset = imp_train, mtry = 736, ntree = ntree)
  rf_models[[as.character(ntree)]] <- rf_model
  
  # Make predictions on the test set
  yhat.rf <- predict(rf_model, newdata = imp_data[-imp_train, ])
  
  # Calculate mean squared error
  mse <- mean((yhat.rf - imp_test)^2)
  mse_results[i] <- mse
}

# Create a data frame for the ntree values and mean squared error
results <- data.frame(ntree = ntree_values, mse = mse_results)

# Create a line plot of the mean squared error
ggplot(results, aes(x = ntree, y = mse)) +
  geom_line() +
  geom_point() +
  xlab("Number of Trees") +
  ylab("Mean Squared Error")

#best is rf with 100 trees, MSE = 20.28067

set.seed(1)
rf.100 <- randomForest(percent_fat ~ ., data = imp_data,
                        subset = imp_train, mtry = 736, ntree = 100, importance = TRUE)
yhat.rf <- predict(rf.100, newdata = imp_data[-imp_train, ])
mean((yhat.rf - imp_test)^2) #MSE = 21.02017

#plotting the important variables 
importance(rf.100)
varImpPlot(rf.100)

#1.d boosted tree 

set.seed(1)
boost.tree <- gbm(percent_fat ~ ., data = imp_data[imp_train, ],
                  distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
summary(boost.tree)
yhat.boost <- predict(boost.tree,
                      newdata = imp_data[-imp_train, ], n.trees = 5000)
mean((yhat.boost - imp_test)^2) #MSE = 24.24829

#changing the shrinkage value 
boost.tree <- gbm(percent_fat ~ ., data = imp_data[imp_train, ], distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.tree,
                      newdata = imp_data[-imp_train, ], n.trees = 5000)
mean((yhat.boost - imp_test)^2) #MSE = 20.77513

#testing n.trees 
#we can see that shrinkage = 0.2 helps 
#n.trees = 100
boost.tree <- gbm(percent_fat ~ ., data = imp_data[imp_train, ], distribution = "gaussian", n.trees = 100, interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.tree,
                      newdata = imp_data[-imp_train, ], n.trees = 100)
mean((yhat.boost - imp_test)^2) #MSE = 19.63405

#trying 500
boost.tree <- gbm(percent_fat ~ ., data = imp_data[imp_train, ], distribution = "gaussian", n.trees = 500, interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.tree,
                      newdata = imp_data[-imp_train, ], n.trees = 500)
mean((yhat.boost - imp_test)^2) #MSE = 31.00306

#making a loop
# Define the range of ntree values to test
n.trees_values <- c(10, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 2000, 5000)

# Create an empty list to store the random forest models
boost_models <- list()
mse_results <- numeric(length = length(n.trees_values))

# Iterate over each ntree value and train the random forest model
set.seed(1)
for (i in seq_along(n.trees_values)) {
  n.trees <- n.trees_values[i]
  
  # Train the random forest model
  boost_model <- gbm(percent_fat ~ ., data = imp_data[imp_train, ], distribution = "gaussian", n.trees = n.trees, interaction.depth = 4, shrinkage = 0.2, verbose = F)
  boost_models[[as.character(n.trees)]] <- boost_model
  
  # Make predictions on the test set
  yhat.boost <- predict(boost_model, newdata = imp_data[-imp_train, ])
  
  # Calculate mean squared error
  mse <- mean((yhat.boost - imp_test)^2)
  mse_results[i] <- mse
}

# Create a data frame for the ntree values and mean squared error
results <- data.frame(n.trees = n.trees_values, mse = mse_results)

# Create a line plot of the mean squared error
ggplot(results, aes(x = n.trees, y = mse)) +
  geom_line() +
  geom_point() +
  xlab("Number of Trees") +
  ylab("Mean Squared Error")

#best n.trees = 900 MSE = 20.43934

#changing the interaction depth 
# Define the range of ntree values to test
int.depth_values <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30)

# Create an empty list to store the random forest models
boost_models <- list()
mse_results <- numeric(length = length(int.depth_values))

# Iterate over each ntree value and train the random forest model
set.seed(1)
for (i in seq_along(int.depth_values)) {
  int.depth <- int.depth_values[i]
  
  # Train the random forest model
  boost_model <- gbm(percent_fat ~ ., data = imp_data[imp_train, ], distribution = "gaussian", n.trees = 100, interaction.depth = int.depth, shrinkage = 0.2, verbose = F)
  boost_models[[as.character(int.depth)]] <- boost_model
  
  # Make predictions on the test set
  yhat.boost <- predict(boost_model, newdata = imp_data[-imp_train, ])
  
  # Calculate mean squared error
  mse <- mean((yhat.boost - imp_test)^2)
  mse_results[i] <- mse
}

# Create a data frame for the ntree values and mean squared error
results <- data.frame(int.depth = int.depth_values, mse = mse_results)

# Create a line plot of the mean squared error
ggplot(results, aes(x = int.depth, y = mse)) +
  geom_line() +
  geom_point() +
  xlab("Interaction Depth") +
  ylab("Mean Squared Error")

#the lowest one is 3: MSE = 20.28067

#changing the shrinkage 
# Define the range of ntree values to test
shrink_values <- c(0.001, 0.01, 0.1, 0.2, 0.3, 1)

# Create an empty list to store the boosted models
boost_models <- list()
mse_results <- numeric(length = length(shrink_values))
set.seed(1)
for (i in seq_along(shrink_values)) {
  shrink <- shrink_values[i]
  
  # Train the random forest model
  boost_model <- gbm(percent_fat ~ ., data = imp_data[imp_train, ], distribution = "gaussian", n.trees = 100, interaction.depth = 3, shrinkage = shrink, verbose = F)
  boost_models[[as.character(shrink)]] <- boost_model
  
  # Make predictions on the test set
  yhat.boost <- predict(boost_model, newdata = imp_data[-imp_train, ])
  
  # Calculate mean squared error
  mse <- mean((yhat.boost - imp_test)^2)
  mse_results[i] <- mse
}

# Create a data frame for the ntree values and mean squared error
results <- data.frame(shrink = shrink_values, mse = mse_results)

# Create a line plot of the mean squared error
ggplot(results, aes(x = shrink, y = mse)) +
  geom_line() +
  geom_point() +
  xlab("Interaction Depth") +
  ylab("Mean Squared Error")
# best shrink = 0.1, MSE = 17.99876

#Bayesian Additive regression tree 
x <- dataset[, 1:2207]
y <- dataset[, "percent_fat"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
yhat.bart <- bartfit$yhat.test.mean
mean((ytest - yhat.bart)^2) #MSE = 83.3126
ord <- order(bartfit$varcount.mean, decreasing = T)
bartfit$varcount.mean[ord]




